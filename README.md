# Q1

This repository contains the implementation of a **Vision Transformer (ViT)** model for image classification.


## Overview

The Vision Transformer (ViT) applies a Transformer architecture, originally designed for NLP, to image classification tasks. It divides images into patches and processes them as a sequence, leveraging self-attention mechanisms to capture global dependencies.

---

## Model Architecture

* Input: Image patches of size `32x32`
* Embedding: Linear projection of flattened patches + positional embeddings
* Transformer Encoder:

  * Multi-head self-attention
  * MLP layers
  * Layer normalization
  * Residual connections
* Classification head: Fully connected layer with softmax activation

---

## Dataset

* Dataset name: `CIFAR10`
* Number of classes: `10`
---

## Run & Check Accuracy



 **Best accuracy tested**

   ```
   Test Accuracy: 67.35%
   ```

---

## Installation

1. Clone the repository:

   ```bash
   git clone <repo-url>
   cd <repo-directory>
   ```
2. install dependencies(for non colab only):

   ```bash
   pip install -r requirements.txt
   ```

---

## Usage

Open the .ipynb file in colab and run all cells

---

## References

* [Dosovitskiy et al., 2020: An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929)

# Q2

## SAM2 Model Usage with Grounding DINO Text Prompts

This guide explains how to use the **pretrained SAM2 (Segment Anything Model 2)** for image segmentation with **text prompts generated by Grounding DINO**, without training the model.

---

## Usage Steps

1. Run all the cells

2. Upload the Image, JPG,JPEG,etc. other support format by 

3. Enter the text prompt.

4. Scroll down to see the whole processing and the final segmentation.

---
